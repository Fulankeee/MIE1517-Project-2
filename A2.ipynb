{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6WDvajSqIDs"
      },
      "source": [
        "# Assignment 2: Hand Gesture Recognition [50 Pt]\n",
        "\n",
        "This assignment will be completed in two parts. In Part A you will gain experience gathering your own data set (specifically images of hand gestures), and understand the challenges involved in the data cleaning process. In Part B you will train a convolutional neural network to make classifications on different hand gestures. By the end of this assignment, you should be able to:\n",
        "\n",
        "1. Generate and preprocess your own data\n",
        "2. Load and split data for training, validation and testing\n",
        "3. Train a Convolutional Neural Network\n",
        "4. Apply transfer learning to improve your model\n",
        "\n",
        "Note that for this assignment we will not be providing you with any starter code. You should be able to take the sample code provided in the preparation sample code, previous assignment, and lectures and modify it accordingly to complete the tasks outlined below.\n",
        "\n",
        "*This assignment is based on an assignment developed by Prof. Lisa Zhang.*\n",
        "\n",
        "### What to submit\n",
        "\n",
        "**Submission for Part A:**  \n",
        "Submit a zip file containing your images. Three images each of American Sign Language gestures for letters A - I (total of 27 images). You will be required to clean the images before submitting them. Details are provided under Part A of the handout.\n",
        "\n",
        "Individual image file names should follow the convention of student-number_Alphabet_file-number.jpg\n",
        "(e.g. 100343434_A_1.jpg).\n",
        "\n",
        "\n",
        "**Submission for Part B:**  \n",
        "Submit an HTML file containing all your code, outputs, and write-up\n",
        "from parts A and B. You can produce a HTML file directly from Google Colab. The Colab instructions are provided at the end of this document.\n",
        "For Part B5(iii), submit a single .csv file following the specified naming and formatting instructions.\n",
        "\n",
        "**Do not submit any other files produced by your code.**\n",
        "\n",
        "Include a link to your colab file in your submission.\n",
        "\n",
        "Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfiFE_WOqIDu"
      },
      "source": [
        "## Colab Link\n",
        "\n",
        "Include a link to your colab file here and ensure the file can be accessed by the our teaching team.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeX7gWbmMdqh"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "# https://colab.research.google.com/github/Fulankeee/MIE1517-Project-2/blob/main/A2.ipynb#scrollTo=X6WDvajSqIDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvTXpH_kqIDy"
      },
      "source": [
        "# Part A. Data Collection [5pt]\n",
        "\n",
        "So far, we have worked with data sets that have been collected, cleaned, and curated by machine learning\n",
        "researchers and practitioners. Datasets like MNIST and CIFAR are often used as toy examples, both by\n",
        "students and by researchers testing new machine learning models.\n",
        "\n",
        "In the real world, getting a clean data set is never that easy. More than half the work in applying machine\n",
        "learning is finding, gathering, cleaning, and formatting your data set.\n",
        "\n",
        "The purpose of this assignment is to help you gain experience gathering your own data set, and understand the\n",
        "challenges involved in the data cleaning process.\n",
        "\n",
        "### American Sign Language\n",
        "\n",
        "American Sign Language (ASL) is a complete, complex language that employs signs made by moving the\n",
        "hands combined with facial expressions and postures of the body. It is the primary language of many\n",
        "North Americans who are deaf and is one of several communication options used by people who are deaf or\n",
        "hard-of-hearing.\n",
        "\n",
        "The hand gestures representing English alphabet are shown below. This assignment focuses on classifying a subset\n",
        "of these hand gesture images using convolutional neural networks. Specifically, given an image of a hand\n",
        "showing one of the letters A-I, we want to detect which letter is being represented.\n",
        "\n",
        "![alt text](https://www.disabled-world.com/pics/1/asl-alphabet.jpg)\n",
        "\n",
        "\n",
        "### Generating Data\n",
        "We will produce the images required to assess our models by ourselves. You are required to collect, clean and submit\n",
        "three images each of Americal Sign Language gestures for letters A - I (total of 27 images)\n",
        "Steps involved in data collection\n",
        "\n",
        "1. Familiarize yourself with American Sign Language gestures for letters from A - I (9 letters).\n",
        "2. Take three pictures at slightly different orientation for each letter gesture using your\n",
        "mobile phone.\n",
        " - Ensure adequate lighting while you are capturing the images.\n",
        " - Use a white wall as your background.\n",
        " - Use your right hand to create gestures (for consistency).\n",
        " - Keep your right hand fairly apart from your body and any other obstructions.\n",
        " - Avoid having shadows on parts of your hand.\n",
        "3. Transfer the images to your laptop for cleaning.\n",
        "\n",
        "### Cleaning Data\n",
        "To simplify the machine learning the task, we will standardize the training images. We will make sure that\n",
        "all our images are of the same size (224 x 224 pixels RGB), and have the hand in the center of the cropped\n",
        "regions.\n",
        "\n",
        "You may use the following applications to crop and resize your images:\n",
        "\n",
        "**Mac**\n",
        "- Use Preview:\n",
        "– Holding down CMD + Shift will keep a square aspect ratio while selecting the hand area.\n",
        "– Resize to 224x224 pixels.\n",
        "\n",
        "**Windows 10**\n",
        "- Use Photos app to edit and crop the image and keep the aspect ratio a square.\n",
        "- Use Paint to resize the image to the final image size of 224x224 pixels.\n",
        "\n",
        "**Linux**\n",
        "- You can use GIMP, imagemagick, or other tools of your choosing.\n",
        "You may also use online tools such as http://picresize.com\n",
        "All the above steps are illustrative only. You need not follow these steps but following these will ensure that\n",
        "you produce a good quality dataset. You will be judged based on the quality of the images alone.\n",
        "Please do not edit your photos in any other way. You should not need to change the aspect ratio of your\n",
        "image. You also should not digitally remove the background or shadows—instead, take photos with a white\n",
        "background and minimal shadows.\n",
        "\n",
        "### Accepted Images\n",
        "Images will be accepted and graded based on the criteria below\n",
        "1. The final image should be size 224x224 pixels (RGB).\n",
        "2. The file format should be a .jpg file.\n",
        "3. The hand should be approximately centered on the frame.\n",
        "4. The hand should not be obscured or cut off.\n",
        "5. The photos follows the ASL gestures posted earlier.\n",
        "6. The photos were not edited in any other way (e.g. no electronic removal of shadows or background).\n",
        "\n",
        "### Submission\n",
        "Submit a zip file containing your images. There should be a total of 27 images (3 for each category)\n",
        "1. Individual image file names should follow the convention of student-number_Alphabet_file-number.jpg\n",
        "(e.g. 100343434_A_1.jpg)\n",
        "2. Zip all the images together and name it with the following convention: last-name_student-number.zip\n",
        "(e.g. last-name_100343434.zip).\n",
        "3. Submit the zipped folder.\n",
        "\n",
        "![alt text](https://github.com/UTNeural/APS360/blob/master/Gesture%20Images.PNG?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxMgWGNqID2"
      },
      "source": [
        "# Part B. Building a CNN [45pt]\n",
        "\n",
        "For this assignment, we are not going to give you any starter code. You will be writing a convolutional neural network\n",
        "from scratch. You are welcome to modify and combine code provided previously in the course (assignments, preparation code, lectures, etc.). You can also write your own code.\n",
        "\n",
        "You may use the PyTorch documentation freely. You might also find online tutorials helpful. However, all code that you submit must be be your own. You cannot take a completed assignment and claim it as your own.\n",
        "\n",
        "Make sure that your code is vectorized, and does not contain obvious inefficiencies (for example, unecessary\n",
        "for loops, or unnecessary calls to unsqueeze()). Ensure enough comments are included in the code so that\n",
        "your TA can understand what you are doing. It is your responsibility to show that you understand what you\n",
        "write.\n",
        "\n",
        "**This is much more challenging and time-consuming than the previous assignments.** Make sure that you give yourself plenty of time by starting early."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiDuQaAh56sT"
      },
      "source": [
        "## Part 1. Data Loading and Splitting [3pt EXPLORATORY]\n",
        "\n",
        "Download the anonymized data provided on Quercus (`A2_Hand_Gesture_Dataset_Revised.zip`). Split the data into training, validation, and test sets. Do not use `A2_Hand_Gesture_Unlabeled_Data.zip` which will be used in later sections.\n",
        "\n",
        "Note: Data splitting is not as trivial in this assignment. We want our test set to closely resemble the setting in which\n",
        "our model will be used. In particular, our test set should contain hands that are never seen in training!\n",
        "\n",
        "Explain how you split the data, either by describing what you did, or by showing the code that you used.\n",
        "Justify your choice of splitting strategy. How many training, validation, and test images do you have?\n",
        "\n",
        "For loading the data, you can use plt.imread, or any other method that you choose. You may find\n",
        "torchvision.datasets.ImageFolder helpful. (see https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WBrH5kBqRLa6"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = !unzip \"/content/A2_Hand_Gesture_Dataset_revised.zip\" -d \"/content/A2_Hand_Gesture_Dataset_revised/\""
      ],
      "metadata": {
        "id": "LE6qaoVqdDSU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loader(data_dir, batch_size):\n",
        "    # Define transformations\n",
        "    transform = transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
        "\n",
        "    # Load the full dataset\n",
        "    dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "    classes = dataset.classes  # Get class names\n",
        "\n",
        "    # Split the dataset\n",
        "    total_size = len(dataset)\n",
        "    test_size = int(0.8 * total_size)\n",
        "    valid_size = int(0.1 * len(dataset))\n",
        "    train_size = total_size - test_size - valid_size\n",
        "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, classes"
      ],
      "metadata": {
        "id": "V-8BqVeL5XSy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_data_loader(data_dir = \"/content/A2_Hand_Gesture_Dataset_revised/Lab_3b_Gesture_Dataset Revised\", batch_size = 32)"
      ],
      "metadata": {
        "id": "arP-fgJh5v77",
        "outputId": "3a07d251-3cd9-45c5-a796-2bd47fb29f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x79e15731d410>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x79e158e08f90>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x79e15731d550>,\n",
              " ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# root = \"/content/A2_Hand_Gesture_Dataset_revised\"\n",
        "# transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
        "# dataset = datasets.ImageFolder(root, transform)\n",
        "\n",
        "# train_size = int(0.8 * len(dataset))\n",
        "# val_size = int(0.1 * len(dataset))\n",
        "# test_size = len(dataset) - train_size - val_size\n",
        "# train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# batch_size = 32\n",
        "\n",
        "# train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "# val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "# test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# print(len(train_loader), len(val_loader), len(test_loader))"
      ],
      "metadata": {
        "id": "r4f75wPYf27D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osN3NBhdNtOC"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "80% Training Data:The majority of the dataset is allocated to training to maximize the model's ability to learn the underlying patterns.\n",
        "10% Validation Data: A smaller portion is used to validate the model's performance during training, tune hyperparameters, and detect overfitting.\n",
        "10% Test Data: The portion left is held to evaluate the model's true performance on unseen data after training.\n",
        "\n",
        "This splitting strategy is an useful approach in deep learning to ensure the model generalizes well without overfitting or underfitting. After splitting, the training loader has\n",
        "122 data points, the validation loader has 11, and the test loader has 11, as well.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VWX4DGY5gQE"
      },
      "source": [
        "## Part 2. Model Building and Sanity Checking [4pt]\n",
        "\n",
        "### Part (i) Convolutional Network [2pt MODEL]\n",
        "\n",
        "Build a convolutional neural network model that takes the (224x224 RGB) image as input, and predicts the gesture\n",
        "letter. Your model should be a subclass of nn.Module. Explain your choice of neural network architecture: how\n",
        "many layers did you choose? What types of layers did you use? Were they fully-connected or convolutional?\n",
        "What about other decisions like pooling layers, activation functions, number of channels / hidden units?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2dtx1z5951fS"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class CNN(nn.Module):\n",
        "#   def __init__(self, input_size, hidden_size, output_size):\n",
        "#     super(CNN, self).__init__()\n",
        "#     self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "#     self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#     self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "#     self.relu = nn.ReLU()\n",
        "#     self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "#     self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = self.pool(self.relu(self.conv1(x)))\n",
        "#     x = self.pool(self.relu(self.conv2(x)))\n",
        "#     x = x.view(x.size(0), -1)\n",
        "#         # Flattens the tensor into a 2D shape suitable for fully connected layers.\n",
        "#         # x.size(0) refers to the batch size, and -1 computes the product of the remaining dimensions.\n",
        "#         # If the tensor shape before this operation is (batch_size, channels, height, width), it becomes (batch_size, channels × height × width).\n",
        "#     x = self.relu(self.fc1(x))\n",
        "#     x = self.fc2(x)\n",
        "#     return x"
      ],
      "metadata": {
        "id": "0mA-1z961zlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.name = \"CNN\"  # Use a different name to avoid conflicts\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(64 * 56 * 56, 32)\n",
        "    self.fc2 = nn.Linear(32, 9)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(self.relu(self.conv1(x)))\n",
        "    x = self.pool(self.relu(self.conv2(x)))\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "5aDgxpyz1XQn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "The architecture has 6 layers in total, which includes 2 convolutional layers (conv1 and conv2); 2 fully connected (linear) layers (fc1 and fc2) and\n",
        "2 pooling layers (pool) that is used after both convolutional layers.\n",
        "ReLU was chosen for its efficiency and ability to avoid the vanishing gradient problem (It replaces negative values with zeros) common with older activation functions (e.g. sigmoid, tanh).\n",
        "\n",
        "For conv1, it has 3 channels. This layer increases the number of feature maps from 3 to 32, enabling it to learn 32 distinct patterns.\n",
        "For conv2, it has 32 channels. This layer increases the number of feature maps from 32 to 64, enabling it to learn 64 distinct patterns.\n",
        "For FC1, it connected layer reduces the high-dimensional input from the convolutional layers to 32 hidden units.\n",
        "* The choice of 32 hidden units in fc1 provides a balance between representational power and avoiding overfitting.\n",
        "For FC2, it connected layer reduces the 32 hidden units to 9 output as there are 9 classes.\n",
        "'''"
      ],
      "metadata": {
        "id": "vSbVXkxM02H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain your choice of neural network architecture: how many layers did you choose? What types of layers did you use? Were they fully-connected or convolutional? What about other decisions like pooling layers, activation functions, number of channels / hidden units?"
      ],
      "metadata": {
        "id": "A_ALDAl80xx_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeGvelvb515e"
      },
      "source": [
        "### Part (ii) Training Code [1pt MODEL]\n",
        "\n",
        "Write code that trains your neural network given some training data. Your training code should make it easy\n",
        "to tweak the usual hyperparameters, like batch size, learning rate, and the model object itself. Make sure\n",
        "that you are checkpointing your models from time to time (the frequency is up to you). Explain your choice\n",
        "of loss function and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "metadata": {
        "id": "y3wx2VWl5CVy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size, learning_rate, epoch)\n",
        "    return path"
      ],
      "metadata": {
        "id": "tfmp3rO-7ETB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(net, loader, criterion):\n",
        "    total_loss = 0.0\n",
        "    total_err = 0.0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad(): # Prevents PyTorch from computing gradients during evaluation.\n",
        "        for inputs, labels in loader:\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute classification error\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_err += (predicted != labels).sum().item()\n",
        "            total_loss += loss.item() # Converts the scalar loss tensor into a Python float.\n",
        "            total_samples += len(labels)\n",
        "\n",
        "    err = total_err / total_samples\n",
        "    avg_loss = total_loss / len(loader)\n",
        "\n",
        "    return err, avg_loss"
      ],
      "metadata": {
        "id": "xu0YoO-87MI2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "bRhFI9RoDX-E",
        "outputId": "c9d003a6-dabd-4479-d2b7-49eebfd86d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Zs8eYG9Oa7Gs"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "def train_net(net, batch_size=32, learning_rate=0.01, num_epochs=5):\n",
        "    ########################################################################\n",
        "    # Train a classifier on an 8-class classification problem\n",
        "    target_classes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"]\n",
        "    ########################################################################\n",
        "    # Fixed PyTorch random seed for reproducible results\n",
        "    torch.manual_seed(1000)\n",
        "    ########################################################################\n",
        "    # Obtain the PyTorch data loader objects to load batches of the datasets\n",
        "    train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "            data_dir='/content/A2_Hand_Gesture_Dataset_revised/Lab_3b_Gesture_Dataset Revised',\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Make sure the classes are as expected\n",
        "    assert len(classes) == 9, \"The dataset should have exactly 9 classes.\"\n",
        "    ########################################################################\n",
        "    # Define the Loss function and optimizer\n",
        "    # Use CrossEntropyLoss for multi-class classification\n",
        "    criterion = nn.CrossEntropyLoss()  # Change to CrossEntropyLoss for multi-class\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "    ########################################################################\n",
        "    # Set up some numpy arrays to store the training/test loss/accuracy\n",
        "    train_err = np.zeros(num_epochs)\n",
        "    train_loss = np.zeros(num_epochs)\n",
        "    val_err = np.zeros(num_epochs)\n",
        "    val_loss = np.zeros(num_epochs)\n",
        "    ########################################################################\n",
        "    # Train the network\n",
        "    # Loop over the data iterator and sample a new batch of training data\n",
        "    # Get the output from the network, and optimize our loss function.\n",
        "    # Each epoch represents one full pass over the training dataset\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "        total_train_loss = 0.0\n",
        "        total_train_err = 0.0\n",
        "        total_epoch = 0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad() # This ensures that gradients from the previous batch are not accumulated.\n",
        "\n",
        "            # Forward pass, backward pass, and optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)  # No need to normalize labels here for CrossEntropyLoss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Calculate the statistics\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest probability\n",
        "            total_train_err += (predicted != labels).sum().item()  # Count errors\n",
        "            total_train_loss += loss.item()\n",
        "            total_epoch += len(labels)\n",
        "\n",
        "        # Calculate error and loss for the training and validation set\n",
        "        train_err[epoch] = float(total_train_err) / total_epoch\n",
        "        train_loss[epoch] = float(total_train_loss) / (i + 1)\n",
        "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
        "\n",
        "        # Print stats for the current epoch\n",
        "        print(f\"Epoch {epoch + 1}: Train err: {train_err[epoch]:.4f}, Train loss: {train_loss[epoch]:.4f} | \"f\"Validation err: {val_err[epoch]:.4f}, Validation loss: {val_loss[epoch]:.4f}\")\n",
        "\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Total time elapsed: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "    # Write the train/test loss/error into CSV files for plotting later\n",
        "    epochs = np.arange(1, num_epochs + 1)\n",
        "    np.savetxt(f\"{model_path}_train_err.csv\", train_err)\n",
        "    np.savetxt(f\"{model_path}_train_loss.csv\", train_loss)\n",
        "    np.savetxt(f\"{model_path}_val_err.csv\", val_err)\n",
        "    np.savetxt(f\"{model_path}_val_loss.csv\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with default set: batch_size=32, learning_rate=0.01, num_epochs=5\n",
        "net = CNN()\n",
        "train_net(net)"
      ],
      "metadata": {
        "id": "_WGryURB9olH",
        "outputId": "3a8ac0dd-b079-4e19-ac31-3b4b89c8a276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train err: 0.8852, Train loss: 3.2410 | Validation err: 0.9218, Validation loss: 2.2421\n",
            "Epoch 2: Train err: 0.8320, Train loss: 2.2045 | Validation err: 0.7984, Validation loss: 2.1406\n",
            "Epoch 3: Train err: 0.7336, Train loss: 2.0355 | Validation err: 0.7449, Validation loss: 1.9421\n",
            "Epoch 4: Train err: 0.6270, Train loss: 1.7437 | Validation err: 0.6543, Validation loss: 1.7607\n",
            "Epoch 5: Train err: 0.5369, Train loss: 1.4932 | Validation err: 0.6008, Validation loss: 1.5966\n",
            "Finished Training\n",
            "Total time elapsed: 224.06 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "kQNirDVYa7Gt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c8b93d10-2130-4e46-e1d8-f24d38c23fd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPROVIDE YOUR ANSWER BELOW\\n\\nThe loss function I choose is \"Cross Entropy Loss\", as it is used for classification (multi-class).\\nThe optimizer I use is Adam as manually tuning the lr is hard, and Adam adjusts the learning rate for each parameter based on the magnitude of its gradients.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "The loss function I choose is \"Cross Entropy Loss\", as it is used for classification (multi-class).\n",
        "The optimizer I use is Adam as manually tuning the lr is hard, and Adam adjusts the learning rate for each parameter based on the magnitude of its gradients.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk1RNgAj54rZ"
      },
      "source": [
        "### Part (iii) “Overfit” to a Small Dataset - [1pt MODEL]\n",
        "\n",
        "One way to sanity check our neural network model and training code is to check whether the model is capable\n",
        "of “overfitting” or “memorizing” a small dataset. A properly constructed CNN with correct training code\n",
        "should be able to memorize the answers to a small number of images quickly.\n",
        "\n",
        "Construct a small dataset (e.g. just the images that you have collected). Then show that your model and\n",
        "training code is capable of memorizing the labels of this small data set.\n",
        "\n",
        "With a large batch size (e.g. the entire small dataset) and learning rate that is not too high, you should be\n",
        "able to obtain a 100% training accuracy on that small dataset relatively quickly (within 200 iterations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "L7rSLxyWbCN3"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "_ = !unzip \"/content/Liu_1006702639.zip\" -d \"/content/Liu_1006702639/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO BE COMPLETED\n",
        "def train_net(net, batch_size=27, learning_rate=0.01, num_epochs=30):\n",
        "    ########################################################################\n",
        "    # Train a classifier on an 8-class classification problem\n",
        "    target_classes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"]\n",
        "    ########################################################################\n",
        "    # Fixed PyTorch random seed for reproducible results\n",
        "    torch.manual_seed(1000)\n",
        "    ########################################################################\n",
        "    # Obtain the PyTorch data loader objects to load batches of the datasets\n",
        "    train_loader, val_loader, test_loader, classes = get_data_loader(\n",
        "            data_dir='/content/Liu_1006702639/Liu_1006702639',\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Make sure the classes are as expected\n",
        "    assert len(classes) == 9, \"The dataset should have exactly 9 classes.\"\n",
        "    ########################################################################\n",
        "    # Define the Loss function and optimizer\n",
        "    # Use CrossEntropyLoss for multi-class classification\n",
        "    criterion = nn.CrossEntropyLoss()  # Change to CrossEntropyLoss for multi-class\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "    ########################################################################\n",
        "    # Set up some numpy arrays to store the training/test loss/accuracy\n",
        "    train_err = np.zeros(num_epochs)\n",
        "    train_loss = np.zeros(num_epochs)\n",
        "    val_err = np.zeros(num_epochs)\n",
        "    val_loss = np.zeros(num_epochs)\n",
        "    ########################################################################\n",
        "    # Train the network\n",
        "    # Loop over the data iterator and sample a new batch of training data\n",
        "    # Get the output from the network, and optimize our loss function.\n",
        "    # Each epoch represents one full pass over the training dataset\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
        "        total_train_loss = 0.0\n",
        "        total_train_err = 0.0\n",
        "        total_epoch = 0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad() # This ensures that gradients from the previous batch are not accumulated.\n",
        "\n",
        "            # Forward pass, backward pass, and optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)  # No need to normalize labels here for CrossEntropyLoss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Calculate the statistics\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the class with the highest probability\n",
        "            total_train_err += (predicted != labels).sum().item()  # Count errors\n",
        "            total_train_loss += loss.item()\n",
        "            total_epoch += len(labels)\n",
        "\n",
        "        # Calculate error and loss for the training and validation set\n",
        "        train_err[epoch] = float(total_train_err) / total_epoch\n",
        "        train_loss[epoch] = float(total_train_loss) / (i + 1)\n",
        "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
        "\n",
        "        # Print stats for the current epoch\n",
        "        print(f\"Epoch {epoch + 1}: Train err: {train_err[epoch]:.4f}, Train loss: {train_loss[epoch]:.4f} | \"f\"Validation err: {val_err[epoch]:.4f}, Validation loss: {val_loss[epoch]:.4f}\")\n",
        "\n",
        "        # Save the current model (checkpoint) to a file\n",
        "        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Total time elapsed: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "    # Write the train/test loss/error into CSV files for plotting later\n",
        "    epochs = np.arange(1, num_epochs + 1)\n",
        "    np.savetxt(f\"{model_path}_train_err.csv\", train_err)\n",
        "    np.savetxt(f\"{model_path}_train_loss.csv\", train_loss)\n",
        "    np.savetxt(f\"{model_path}_val_err.csv\", val_err)\n",
        "    np.savetxt(f\"{model_path}_val_loss.csv\", val_loss)"
      ],
      "metadata": {
        "id": "_47fNebZInZV"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on small dataset with set: batch_size=27, learning_rate=0.01, num_epochs=30\n",
        "net = CNN()\n",
        "train_net(net, batch_size=27, learning_rate=0.01, num_epochs=30)"
      ],
      "metadata": {
        "id": "Vjs12z5iJPXo",
        "outputId": "2fb10507-28dc-4c8f-dd07-5ae360ef1ccf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train err: 1.0000, Train loss: 2.2575 | Validation err: 1.0000, Validation loss: 13.8772\n",
            "Epoch 2: Train err: 0.5000, Train loss: 5.5460 | Validation err: 1.0000, Validation loss: 10.1939\n",
            "Epoch 3: Train err: 0.5000, Train loss: 2.7789 | Validation err: 1.0000, Validation loss: 6.4747\n",
            "Epoch 4: Train err: 0.7500, Train loss: 1.2835 | Validation err: 1.0000, Validation loss: 5.8832\n",
            "Epoch 5: Train err: 0.7500, Train loss: 1.4918 | Validation err: 1.0000, Validation loss: 5.0201\n",
            "Epoch 6: Train err: 0.7500, Train loss: 1.2214 | Validation err: 1.0000, Validation loss: 4.4660\n",
            "Epoch 7: Train err: 0.2500, Train loss: 1.0897 | Validation err: 1.0000, Validation loss: 4.6153\n",
            "Epoch 8: Train err: 0.2500, Train loss: 1.0423 | Validation err: 1.0000, Validation loss: 4.6339\n",
            "Epoch 9: Train err: 0.5000, Train loss: 0.9303 | Validation err: 1.0000, Validation loss: 5.2293\n",
            "Epoch 10: Train err: 0.5000, Train loss: 0.8706 | Validation err: 1.0000, Validation loss: 5.5215\n",
            "Epoch 11: Train err: 0.5000, Train loss: 0.7262 | Validation err: 1.0000, Validation loss: 6.0123\n",
            "Epoch 12: Train err: 0.2500, Train loss: 0.6033 | Validation err: 1.0000, Validation loss: 6.8368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvDLw-Vz6eVS"
      },
      "source": [
        "## Part 3. Hyperparameter Search [7pt]\n",
        "\n",
        "### Part (i) - [2pt EXPLORATORY]\n",
        "\n",
        "List 3 hyperparameters that you think are most worth tuning. Choose at least one hyperparameter related to\n",
        "the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q04SaRRjbFCR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeD6EzPB6kSW"
      },
      "source": [
        "### Part (ii) - [1pt MODEL]\n",
        "\n",
        "Tune the hyperparameters you listed in Part (i), trying as many values as you need to until you feel satisfied\n",
        "that you are getting a good model. Plot the training curve of at least 4 different hyperparameter settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDRiNGW8bJ-y"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H93iN5_l60BO"
      },
      "source": [
        "### Part (iii) - [2pt DISCUSSION]\n",
        "Choose the best model out of all the ones that you have trained. Justify your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJHwissDbNMD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzNA5oup67JO"
      },
      "source": [
        "### Part (iv) - [2pt RESULTS]\n",
        "Report the test accuracy of your best model. You should only do this step once and prior to this step you should have only used the training and validation data.\n",
        "\n",
        "Additionally, provide a confusion matrix to visualize the performance of your model across different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL_gly-PbOyD"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6R_NNH_bOyN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrem-iXV6_Bz"
      },
      "source": [
        "## Part 4. Transfer Learning [8pt]\n",
        "For many image classification tasks, it is generally not a good idea to train a very large deep neural network\n",
        "model from scratch due to the enormous compute requirements and lack of sufficient amounts of training\n",
        "data.\n",
        "\n",
        "One of the better options is to try using an existing model that performs a similar task to the one you need\n",
        "to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed **Transfer\n",
        "Learning**. In this assignment, we will use Transfer Learning to extract features from the hand gesture\n",
        "images. Then, train a smaller network to use these features as input and classify the hand gestures.\n",
        "\n",
        "As you have learned from the CNN lecture, convolution layers extract various features from the images which\n",
        "get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal\n",
        "role in establishing Deep Neural Nets as a go-to tool for image classification problems and we will use an\n",
        "ImageNet pre-trained AlexNet model to extract features in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWdQJz4Q7O2F"
      },
      "source": [
        "### Part (i) - [1pt EXPLORATORY]\n",
        "Here is the code to load the AlexNet network, with pretrained weights. When you first run the code, PyTorch\n",
        "will download the pretrained weights from the internet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJKcTW9C7TZk"
      },
      "outputs": [],
      "source": [
        "import torchvision.models\n",
        "alexnet = torchvision.models.alexnet(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ0GZYaP7VAR"
      },
      "source": [
        "The alexnet model is split up into two components: *alexnet.features* and *alexnet.classifier*. The\n",
        "first neural network component, *alexnet.features*, is used to compute convolutional features, which are\n",
        "taken as input in *alexnet.classifier*.\n",
        "\n",
        "The neural network alexnet.features expects an image tensor of shape Nx3x224x224 as input and it will\n",
        "output a tensor of shape Nx256x6x6 . (N = batch size).\n",
        "\n",
        "Compute the AlexNet features for each of your training, validation, and test data. Here is an example code\n",
        "snippet showing how you can compute the AlexNet features for some images (your actual code might be\n",
        "different):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX7SjVdB7XAE"
      },
      "outputs": [],
      "source": [
        "# img = ... a PyTorch tensor with shape [N,3,224,224] containing hand images ...\n",
        "features = alexnet.features(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYcjHg_A7cCM"
      },
      "source": [
        "**Save the computed features**. You will be using these features as input to your neural network in Part\n",
        "(ii), and you do not want to re-compute the features every time. Instead, run *alexnet.features* once for\n",
        "each image, and save the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDQr5eBybSOL"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFWvvhFN73qY"
      },
      "source": [
        "### Part (ii) - [1pt MODEL]\n",
        "Build a convolutional neural network that takes as input these AlexNet features, and makes a prediction using additional convolutional and/or linear layers. Your model should be a subclass of nn.Module.\n",
        "\n",
        "Explain your choice of neural network architecture: how many layers did you choose? What types of layers\n",
        "did you use: fully-connected or convolutional? What about other decisions like pooling layers, activation\n",
        "functions, number of channels / hidden units in each layer?\n",
        "\n",
        "Here is an example of how your model may be called:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVTuHUeV78-U"
      },
      "outputs": [],
      "source": [
        "# features = ... load precomputed alexnet.features(img) ...\n",
        "output = model(features)\n",
        "prob = F.softmax(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0fiyAZVbapt"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7FVHuYkbaqF"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVAGuURu7-9q"
      },
      "source": [
        "### Part (iii) - [1pt MODEL]\n",
        "Train your new network, including any hyperparameter tuning. Plot and submit the training curve of your\n",
        "best model only.\n",
        "\n",
        "Note: Depending on how you are caching (saving) your AlexNet features, PyTorch might still be tracking\n",
        "updates to the **AlexNet weights**, which we are not tuning. One workaround is to convert your AlexNet\n",
        "feature tensor into a numpy array, and then back into a PyTorch tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCmiH11x7-q1"
      },
      "outputs": [],
      "source": [
        "tensor = torch.from_numpy(tensor.detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbGXEv_9bd6_"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ2tvqJ68Mqb"
      },
      "source": [
        "### Part (iv) - [1pt RESULTS]\n",
        "Report the test accuracy of your best model. How does the test accuracy compare to Part 3(iv) without transfer learning?\n",
        "\n",
        "Additionally, provide a confusion matrix to visualize the performance of your model across different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7TJTa_qbfrR"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fyh08uCbfrm"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (v) - Transfer Learning with Another Pretrained Model [1pts EXPLORATORY]\n",
        "In this section, you will repeat the steps performed in Parts B4(i) to B4(iv) using another pretrained model available in torchvision.models. The goal is to compare the results obtained with AlexNet to those obtained with another model, such as VGG16 or ResNet18 (you can choose your model/s of interest)."
      ],
      "metadata": {
        "id": "OlOMAeplTOfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the model/s of interest\n",
        "## TO BE COMPLETED\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p4TvTSBJXw7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExuGaPASYYaF"
      },
      "source": [
        "### Part (vi) - [1pt MODEL]\n",
        "Build a convolutional neural network model that makes\n",
        "predictions based on features extracted by your model of interest in Part B4(v). Your model should be a subclass of nn.Module.\n",
        "\n",
        "Explain your choice of neural network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBXTafjHYzul"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QukvaphlZB-O"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (vii) - [1pt MODEL]\n",
        "Train your new network, including any hyperparameter tuning. Plot and submit the training curve of your best model only."
      ],
      "metadata": {
        "id": "ZwzxSiwiZWKH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtvERFJeZfeU"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (viii) - [1pt RESULTS]\n",
        "Compare the performance of AlexNet (Part B4(iv)) and the other pretrained model you selected (Part B4(vii)). Discuss the differences in performance between AlexNet and the new model."
      ],
      "metadata": {
        "id": "XL4c2TaqaEtB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oowtpuLjaUjX"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnmMcouh-UFm"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrDPb0orGZYJ"
      },
      "source": [
        "## Part 5. Testing on New Data [14pt]\n",
        "\n",
        "In this final step, we will evaluate the models trained using new data that was not part of the training process. In this assignment, there will be three different test datasets:\n",
        "\n",
        "1. The test dataset generated by splitting the labeled data provided in the file `A2_Hand_Gesture_Dataset_revised.zip` into train, validation, and test sets. It is used throughout Part B1 to B4.\n",
        "2. The data you collected in Part A.\n",
        "3. The unlabeled data provided in `A2_Hand_Gesture_Unlabeled_Data.zip`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsunI_7ILqnS"
      },
      "source": [
        "### Part (i) - [2pt EXPLORATORY]\n",
        "Load and process the hand gesture images you had collected so that they can easily be evaulated by your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSBa_pDmblQZ"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z7JBhPmLqnS"
      },
      "source": [
        "### Part (ii) - [2pt RESULTS]\n",
        "Using the best transfer learning model developed in Part 4. Report the test accuracy on your sample images and how it compares to the test accuracy obtained in Part 4(iv)? Also, use your best model from Part 3 to report the test accuracy on the new sample images and how it compares to the test accuracy obtained in Part 3(iv). You should present your results in a pandas DataFrame.\n",
        "\n",
        "This question will also be evaluated on the overal performance achieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfT77JP3bpud"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part (iii) - [3pt RESULTS]\n",
        "\n",
        "Load the unlabeled hand gesture images from the supplementary assignment files on Quercus (`A2_Hand_Gesture_Unlabeled_Data.zip`), which contains 615 unlabeled images. Using the best transfer learning model developed in Part 4, classify all 615 images and save the results in a .CSV file with the following format (Note: The file will be evaluated using an automated Python script to assess your model's performance, so make sure to follow the format precisely).\n",
        "\n",
        "1. The .csv file must contain exactly 615 lines.\n",
        "2. Each line should have two comma-separated values: the first value is the image ID number (the filename, e.g., 1 or 123), and the second is the image class (a letter, such as A, B, C, D, E, F, G, H, or I). Do not use spaces or quotation marks.\n",
        "3. Name your .csv file your student number (Example: 1001234567.csv)\n",
        "\n",
        "Example .csv file:\n",
        "```\n",
        "1,A\n",
        "2,B\n",
        "3,C\n",
        "4,H\n",
        "...\n",
        "..\n",
        "615,I\n",
        "```"
      ],
      "metadata": {
        "id": "ZtWjvcTJd_ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##TO BE COMPLETED\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9X1_E4iroG7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBdVwrmGmdS0"
      },
      "source": [
        "### Part (iv) - [5pt DISCUSSION]\n",
        "How well did your models do for the different hand guestures? Provide an explanation for why you think your models performed the way it did? Be sure to compare the results for three test datasets mentioned in part B5. You can refer to the confusion matrices presented earlier.\n",
        "\n",
        "How well would this model perform if you were to deploy it in the real-world? List some things you could do to improve the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD5YZTfBbpu1"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyCMx83xLJt0"
      },
      "source": [
        "### Part (v) - [2pt DISCUSSION]\n",
        "Reflect on the ethical implications of deploying a hand gesture recognition model in real-world applications. What are some potential biases that could arise in your model, and how might they impact different user groups?\n",
        "\n",
        "Discuss measures you can take to identify and mitigate these biases during the development and deployment phases. Consider aspects such as data collection, model training, and user feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFsB8SN0LJt0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SMqukISPXen"
      },
      "source": [
        "# PART C (Optional) - Bonus Challenge!\n",
        "\n",
        "This is an optional exercise for those that finish the assignment early and would like to take on some additional challenging task.\n",
        "\n",
        "For this bonus challenge you are asked to explore different techniques to further improve the model for more realistic scenarios where the hands may be in front of various backgrounds, be taken at different scales, rotations, and positions in the image. To build a superior image classification model you can complete one or all of the following tasks.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Test your best models (with or without transfer learning) on hand gestures images with backgrounds that are not clean and smooth. You can take your own hand gesture images, or find ones online to test your model. Comment on the performance of your model.\n",
        "2. Generated additional data by performing data augmentation to replace the clean, smooth backgrounds with backgrounds with different textures. Do this on all your training, validation, and test data. This task can be done manually (could take a long time) or you can implement code that can automate the task. For Example, one approach you could take to automate this task is to use pretrained image segmentation tools to identify the hands in the images and crop them out and put them onto other backgrounds.\n",
        "3. Implement one of many region proposal tools to detect hand gestures in image frames and put a bounding box around the region with the highest confidence. You can use pretrained YOLO models, or a region proposal tool from openCV.\n",
        "4. Create a short video demonstration how you can use your hand gesture recognition model to communicate a message using hand gestures.\n",
        "\n",
        "Bonus marks will be provided based on the number of steps completed. Summarize below your results and anything intersting you learned from the steps that you completed. Bonus marks cannot be accumulated beyond the maximum assignment grade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7RdCepbPbya"
      },
      "outputs": [],
      "source": [
        "# TO BE COMPLETED\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-16xaXFPcql"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PROVIDE YOUR ANSWER BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYwI4RmFS2RB"
      },
      "source": [
        "### Saving to HTML\n",
        "Detailed instructions for saving to HTML can be found <a href=\"https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab/64487858#64487858\">here</a>. Provided below are a summary of the instructions:\n",
        "\n",
        "(1) download your ipynb file by clicking on File->Download.ipynb\n",
        "\n",
        "(2) reupload your file to the temporary Google Colab storage (you can access the temporary storage from the tab to the left)\n",
        "\n",
        "(3) run the following:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aW8Kd8kwC5oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TrsqdNgS5ex"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/drive/MyDrive/MIE1517-TA-2025/A2.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuXhlFlPTY7F"
      },
      "source": [
        "(4) the html file will be available for download in the temporary Google Colab storage\n",
        "\n",
        "(5) review the html file and make sure all the results are visible before submitting your assignment to Quercus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLzCpNFMhgxO"
      },
      "source": [
        "# Assignment Grading Rubric\n",
        "The grading of the assignment will be based on the following categories:\n",
        "\n",
        "(0) **5 Pt - DATA** Generating new data to evaluate your model.\n",
        "\n",
        "(1) **9 Pt - EXPLORATORY QUESTIONS** These are basic questions that in most cases can be answered without requiring a fully working and trained neural network model. For example, data loading, processing and visualization, summary statistics, data exploration, model and training setup, etc.\n",
        "\n",
        "(2) **9 Pt - MODEL** Student has successfully implemented all the required neural network models and has demonstrated successful training of the model without any errors.\n",
        "\n",
        "(3) **9 Pt - RESULT** Students are evaluated based on the results achieved in comparison to the expected results of the assignment.\n",
        "\n",
        "(4) **9 Pt - DISCUSSION QUESTIONS** Student demonstrated understanding beyond the basic exploratory questions, can answer some of the more challenging questions, and provide arguments for their model selection decisions.\n",
        "\n",
        "(5) **9 Pt - COMMUNICATION** Student has provided a quality submission that is easy to read without too many unnecessary output statements that distract the reading of the document. The code has been well commented and all the answers are communicated clearly and concisely.\n",
        "\n",
        "(6) **5 Pt - BONUS** Student has completed the assignment and has taken on the challenging bonus tasks listed in PART C. The student has demonstrated a good understanding of all aspects of the assignment and has exceeded expectations for the assignment.\n",
        "\n",
        "\n",
        "\n",
        "**TOTAL GRADE = _____ of 50 Pts**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}